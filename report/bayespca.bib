@InProceedings{bishop1999bayesian,
author = {Bishop, Christopher},
title = {Bayesian PCA},
booktitle = {Advances in Neural Information Processing Systems},
year = {1999},
month = {January},
abstract = {The technique of principal component analysis (PCA) has recently been expressed as the maximum likelihood solution for a generative latent variable model. In this paper we use this probabilistic reformulation as the basis for a Bayesian treatment of PCA. Our key result is that effective dimensionality of the latent space (equivalent to the number of retained principal components) can be determined automatically as part of the Bayesian inference procedure. An important application of this framework is to mixtures of probabilistic PCA models, in which each component can determine its own effective complexity.},
publisher = {MIT Press},
url = {https://www.microsoft.com/en-us/research/publication/bayesian-pca/},
pages = {382-388},
volume = {11},
edition = {Advances in Neural Information Processing Systems},
}

@Article{tipping1999mixtures,
author = {Tipping, M. E. and Bishop, Christopher},
title = {Mixtures of Probabilistic Principal Component Analyzers},
year = {1999},
month = {January},
abstract = {Principal component analysis (PCA) is one of the most popular techniques for processing, compressing and visualising data, although its effectiveness is limited by its global linearity. While nonlinear variants of PCA have been proposed, an alternative paradigm is to capture data complexity by a combination of local linear PCA projections. However, conventional PCA does not correspond to a probability density, and so there is no unique way to combine PCA models. Previous attempts to formulate mixture models for PCA have therefore to some extent been ad hoc. In this paper, PCA is formulated within a maximum-likelihood framework, based on a specific form of Gaussian latent variable model. This leads to a well-defined mixture model for probabilistic principal component analysers, whose parameters can be determined using an EM algorithm. We discuss the advantages of this model in the context of clustering, density modelling and local dimensionality reduction, and we demonstrate its application to image compression and handwritten digit recognition.},
url = {https://www.microsoft.com/en-us/research/publication/mixtures-of-probabilistic-principal-component-analyzers/},
pages = {443-482},
journal = {Neural Computation},
volume = {11},
edition = {Neural Computation},
}

@article{tipping1999ppca,
 ISSN = {13697412, 14679868},
 URL = {http://www.jstor.org/stable/2680726},
 abstract = {Principal component analysis (PCA) is a ubiquitous technique for data analysis and processing, but one which is not based on a probability model. We demonstrate how the principal axes of a set of observed data vectors may be determined through maximum likelihood estimation of parameters in a latent variable model that is closely related to factor analysis. We consider the properties of the associated likelihood function, giving an EM algorithm for estimating the principal subspace iteratively, and discuss, with illustrative examples, the advantages conveyed by this probabilistic approach to PCA.},
 author = {Michael E. Tipping and Christopher M. Bishop},
 journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
 number = {3},
 pages = {611--622},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Probabilistic Principal Component Analysis},
 volume = {61},
 year = {1999}
}
